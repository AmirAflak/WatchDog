{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b46fcd-bfc8-4ca1-ba06-eaa5b66c9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c260917-818d-46cc-82a0-29ef43a44252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mongodb import MongoDBClient\n",
    "from configs import BOOTSTRAP_SERVERS, KAFKA_TOPIC, MONGO_HOST, MONGO_PORT, MONGO_DB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef778bbc-5000-4432-b55e-9821bfa53fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/mehrdad/anaconda3/envs/DE/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mehrdad/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mehrdad/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e0ca819a-2661-439e-b091-1db93b397d67;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.2/spark-sql-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2!spark-sql-kafka-0-10_2.12.jar (50ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.2/spark-token-provider-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2!spark-token-provider-kafka-0-10_2.12.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.6.0/kafka-clients-2.6.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.6.0!kafka-clients.jar (311ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (39ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.8-1/zstd-jni-1.4.8-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.8-1!zstd-jni.jar (343ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (57ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (78ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (14ms)\n",
      ":: resolution report :: resolve 5625ms :: artifacts dl 979ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   9   |   9   |   0   ||   9   |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e0ca819a-2661-439e-b091-1db93b397d67\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (13083kB/44ms)\n",
      "23/08/26 19:33:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkMongoDB\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e92f4b6-ce0b-4adb-8b38-378a18301fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"key\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbacdc9d-8231-4ff7-8410-733fcd3b7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoDBClient(MONGO_HOST, MONGO_PORT, MONGO_DB_NAME, username='admin', password='password')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9ddd18b-5681-4a3d-aa5b-b28b3598aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"subscribe\", \"subs\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "400c04b6-ade0-4a31-ac98-d072a7671353",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f28d8a1e-5acb-45f4-9342-3e8b7c10f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_value = lambda x: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "579405eb-1990-44cd-9786-1d5b56fbc8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'value'>\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN] Argument `col` should be a Column, got NoneType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mparsed_df\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DE/lib/python3.10/site-packages/pyspark/sql/dataframe.py:4785\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   4742\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4743\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[1;32m   4744\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4782\u001b[0m \u001b[38;5;124;03m+---+-----+----+\u001b[39;00m\n\u001b[1;32m   4783\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m-> 4785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   4786\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4787\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   4788\u001b[0m     )\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN] Argument `col` should be a Column, got NoneType."
     ]
    }
   ],
   "source": [
    "processed_df = parsed_df \\\n",
    "    .withColumn(\"processed_value\", show_value(col(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07e2231d-1708-45d3-97fc-4a36ed6d5a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string, timestamp: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69b803ef-ac6b-4a81-b509-c462c04c6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, epoch_id):\n",
    "    if not df.isEmpty():\n",
    "    # Convert DataFrame to RDD\n",
    "        rdd = df.rdd\n",
    "    if not rdd.isEmpty():\n",
    "    # Process each message in the RDD\n",
    "        for row in rdd.collect():\n",
    "    # Example: assume each Kafka message is a JSON document\n",
    "    # Insert the document into MongoDB\n",
    "            print(row.asDict())\n",
    "    # collection.insert_one(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1affdc9d-d6e7-4bed-a257-ad57a1e343a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/26 21:03:38 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-771038dc-1627-4b48-afba-1509cbb2a1f8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/08/26 21:03:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/08/26 21:03:38 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 21:03:38 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 21:03:38 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/08/26 21:03:38 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/08/26 21:03:38 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/08/26 21:03:39 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:03:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': None, 'value': bytearray(b'\"\"'), 'topic': 'subs', 'partition': 0, 'offset': 0, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 752000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwaws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 1, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 874000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 2, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 880000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa2.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 3, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 890000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"china.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 4, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 894000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"m.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 5, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 900000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 6, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 903000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.usa.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 7, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 908000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.reports.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 8, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 912000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"new-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 9, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 915000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 10, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 919000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa-staging.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 11, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 923000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwdev.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 12, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 927000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"w.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 13, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 930000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"staging-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 14, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 938000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"ay4omq1chacs-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 15, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 942000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwuat.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 16, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 950000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"reports.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 17, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 955000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 18, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 960000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"sustainabilityart.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 19, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 968000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"careers.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 20, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 974000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwawsqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 21, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 979000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"strategy.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 22, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 988000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"investors.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 23, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 22, 997000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"dealertechjobs.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 24, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 23, 4000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"0q91frjc734c-wwwqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 25, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 23, 13000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwawsqa.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 26, 'timestamp': datetime.datetime(2023, 8, 26, 19, 32, 23, 21000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"ir.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 27, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 508000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"2a.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 28, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 515000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"staging-www.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 29, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 534000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"content.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 30, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 555000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 31, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 574000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"investor.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 32, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 585000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.china.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 33, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 614000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"caterpillar.com.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 34, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 626000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"com.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 35, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 649000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 36, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 654000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"visitcaterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 37, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 669000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.visitcaterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 38, 'timestamp': datetime.datetime(2023, 8, 26, 19, 33, 3, 681000), 'timestampType': 0}\n"
     ]
    }
   ],
   "source": [
    "query = df.writeStream.foreachBatch(process_batch).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cff96e-bded-451b-9ccf-a71e4e47b83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/26 21:09:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:56 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:56 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:56 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:56 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:56 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:57 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:58 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:09:58 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'\"\"'), topic='subs', partition=0, offset=39, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 794000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwdev.aws.caterpillar.com\"'), topic='subs', partition=0, offset=40, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 819000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"reports.caterpillar.com\"'), topic='subs', partition=0, offset=41, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 832000), timestampType=0)\n",
      "{'key': None, 'value': bytearray(b'\"\"'), 'topic': 'subs', 'partition': 0, 'offset': 39, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 794000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwdev.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 40, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 819000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"reports.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 41, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 832000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"\"'), 'topic': 'subs', 'partition': 0, 'offset': 39, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 794000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwdev.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 40, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 819000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"reports.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 41, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 832000), 'timestampType': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:10:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'\"china.caterpillar.com\"'), topic='subs', partition=0, offset=42, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 836000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"w.caterpillar.com\"'), topic='subs', partition=0, offset=43, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 840000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"ay4omq1chacs-www.caterpillar.com\"'), topic='subs', partition=0, offset=44, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 852000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"0q91frjc734c-wwwqaext.caterpillar.com\"'), topic='subs', partition=0, offset=45, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 861000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"strategy.caterpillar.com\"'), topic='subs', partition=0, offset=46, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 884000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"dealertechjobs.caterpillar.com\"'), topic='subs', partition=0, offset=47, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 896000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwawsqaext.caterpillar.com\"'), topic='subs', partition=0, offset=48, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 915000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"new-www.caterpillar.com\"'), topic='subs', partition=0, offset=49, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 928000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwqa-staging.aws.caterpillar.com\"'), topic='subs', partition=0, offset=50, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 936000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwqa2.aws.caterpillar.com\"'), topic='subs', partition=0, offset=51, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 944000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwqa.aws.caterpillar.com\"'), topic='subs', partition=0, offset=52, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 957000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwawsqa.caterpillar.com\"'), topic='subs', partition=0, offset=53, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 961000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"m.caterpillar.com\"'), topic='subs', partition=0, offset=54, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 970000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwqaext.caterpillar.com\"'), topic='subs', partition=0, offset=55, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 975000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"www.usa.caterpillar.com\"'), topic='subs', partition=0, offset=56, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 982000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"careers.caterpillar.com\"'), topic='subs', partition=0, offset=57, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 987000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"staging-www.caterpillar.com\"'), topic='subs', partition=0, offset=58, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 52, 995000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwaws.caterpillar.com\"'), topic='subs', partition=0, offset=59, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 53, 20000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"www.caterpillar.com\"'), topic='subs', partition=0, offset=60, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 53, 46000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"investors.caterpillar.com\"'), topic='subs', partition=0, offset=61, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 53, 53000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"www.reports.caterpillar.com\"'), topic='subs', partition=0, offset=62, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 53, 58000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"caterpillar.com\"'), topic='subs', partition=0, offset=63, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 53, 75000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"wwwuat.aws.caterpillar.com\"'), topic='subs', partition=0, offset=64, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 53, 99000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"sustainabilityart.caterpillar.com\"'), topic='subs', partition=0, offset=65, timestamp=datetime.datetime(2023, 8, 26, 21, 9, 53, 113000), timestampType=0)\n",
      "{'key': None, 'value': bytearray(b'\"china.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 42, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 836000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"w.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 43, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 840000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"ay4omq1chacs-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 44, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 852000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"0q91frjc734c-wwwqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 45, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 861000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"strategy.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 46, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 884000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"dealertechjobs.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 47, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 896000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwawsqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 48, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 915000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"new-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 49, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 928000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa-staging.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 50, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 936000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa2.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 51, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 944000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 52, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 957000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwawsqa.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 53, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 961000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"m.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 54, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 970000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 55, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 975000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.usa.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 56, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 982000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"careers.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 57, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 987000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"staging-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 58, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 995000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwaws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 59, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 20000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 60, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 46000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"investors.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 61, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 53000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.reports.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 62, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 58000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 63, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 75000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwuat.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 64, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 99000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"sustainabilityart.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 65, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 113000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"china.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 42, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 836000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"w.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 43, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 840000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"ay4omq1chacs-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 44, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 852000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"0q91frjc734c-wwwqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 45, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 861000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"strategy.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 46, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 884000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"dealertechjobs.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 47, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 896000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwawsqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 48, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 915000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"new-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 49, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 928000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa-staging.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 50, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 936000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa2.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 51, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 944000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqa.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 52, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 957000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwawsqa.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 53, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 961000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"m.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 54, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 970000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwqaext.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 55, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 975000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.usa.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 56, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 982000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"careers.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 57, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 987000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"staging-www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 58, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 52, 995000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwaws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 59, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 20000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 60, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 46000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"investors.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 61, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 53000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.reports.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 62, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 58000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 63, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 75000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"wwwuat.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 64, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 99000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"sustainabilityart.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 65, 'timestamp': datetime.datetime(2023, 8, 26, 21, 9, 53, 113000), 'timestampType': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/26 21:11:08 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:10 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': None, 'value': bytearray(b'\"investor.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 66, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 796000), 'timestampType': 0}\n",
      "Row(key=None, value=bytearray(b'\"investor.caterpillar.com\"'), topic='subs', partition=0, offset=66, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 796000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"2a.caterpillar.com\"'), topic='subs', partition=0, offset=67, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 801000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"staging-www.aws.caterpillar.com\"'), topic='subs', partition=0, offset=68, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 810000), timestampType=0)\n",
      "{'key': None, 'value': bytearray(b'\"investor.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 66, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 796000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"2a.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 67, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 801000), 'timestampType': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:11 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': None, 'value': bytearray(b'\"2a.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 67, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 801000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"staging-www.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 68, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 810000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"content.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 69, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 819000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 70, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 826000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 71, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 830000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"caterpillar.com.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 72, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 836000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"ir.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 73, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 839000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"com.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 74, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 844000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.china.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 75, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 848000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"visitcaterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 76, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 852000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.visitcaterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 77, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 855000), 'timestampType': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/26 21:11:12 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'\"content.caterpillar.com\"'), topic='subs', partition=0, offset=69, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 819000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"aws.caterpillar.com\"'), topic='subs', partition=0, offset=70, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 826000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"www.aws.caterpillar.com\"'), topic='subs', partition=0, offset=71, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 830000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"caterpillar.com.caterpillar.com\"'), topic='subs', partition=0, offset=72, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 836000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"ir.caterpillar.com\"'), topic='subs', partition=0, offset=73, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 839000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"com.caterpillar.com\"'), topic='subs', partition=0, offset=74, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 844000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"www.china.caterpillar.com\"'), topic='subs', partition=0, offset=75, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 848000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"visitcaterpillar.com\"'), topic='subs', partition=0, offset=76, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 852000), timestampType=0)\n",
      "Row(key=None, value=bytearray(b'\"www.visitcaterpillar.com\"'), topic='subs', partition=0, offset=77, timestamp=datetime.datetime(2023, 8, 26, 21, 11, 7, 855000), timestampType=0)\n",
      "{'key': None, 'value': bytearray(b'\"staging-www.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 68, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 810000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"content.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 69, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 819000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 70, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 826000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.aws.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 71, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 830000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"caterpillar.com.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 72, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 836000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"ir.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 73, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 839000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"com.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 74, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 844000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.china.caterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 75, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 848000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"visitcaterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 76, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 852000), 'timestampType': 0}\n",
      "{'key': None, 'value': bytearray(b'\"www.visitcaterpillar.com\"'), 'topic': 'subs', 'partition': 0, 'offset': 77, 'timestamp': datetime.datetime(2023, 8, 26, 21, 11, 7, 855000), 'timestampType': 0}\n"
     ]
    }
   ],
   "source": [
    "print('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36998b5e-7f28-4995-893c-4df46918096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"com.caterpillar.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a76fb492-5cb5-4244-a994-9426493f3f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['com', '']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/26 22:03:51 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:51 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:52 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:52 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:52 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:53 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:53 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:53 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:54 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:54 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:55 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:55 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:55 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:56 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:56 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:56 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:57 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:57 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:58 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:58 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:00 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:00 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:00 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:01 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:01 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:01 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:02 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:02 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:02 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:03 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:03 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:03 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:04 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:04 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:04 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:05 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:05 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:05 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:06 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:06 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:07 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:07 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:07 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:08 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:08 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:08 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:08 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:09 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:09 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:10 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:10 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:10 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:10 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:11 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:11 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:12 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:12 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:12 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:13 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:13 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:13 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:14 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:14 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:14 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:15 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:15 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:16 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:16 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:16 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:17 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:17 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:17 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:18 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:18 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:18 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:19 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:19 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:19 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:20 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:20 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:20 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:21 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:21 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:21 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:22 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:22 WARN NetworkClient: [AdminClient clientId=adminclient-3] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:22 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1001 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/08/26 22:04:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:96)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:325)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:511)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:96)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:325)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:511)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/08/26 22:04:34 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:34 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:35 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:35 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:35 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:35 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/08/26 22:04:35 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/08/26 22:04:35 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/08/26 22:04:36 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:36 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:36 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:37 ERROR MicroBatchExecution: Query [id = e8243a54-ade0-44c4-bb9c-7a8bbcb3a571, runId = 7a7f73c7-5d07-4076-829b-76a7489b1551] terminated with error\n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:37 ERROR MicroBatchExecution: Query [id = c974be6a-a617-4cd9-8d8d-07cff0bab89a, runId = 20af81b2-b3d6-4540-8da6-2c4e710d0784] terminated with error\n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "23/08/26 22:04:37 ERROR MicroBatchExecution: Query [id = 470bba70-87d6-4b97-aac6-411bd55480ef, runId = 5b57017b-4d15-4731-8265-79ef7659b5a4] terminated with error\n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderAdmin.scala:537)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetriesWithoutInterrupt(KafkaOffsetReaderAdmin.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToConsumer(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:289)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:489)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:488)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:477)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n"
     ]
    }
   ],
   "source": [
    "a.split('.caterpillar.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbe64b-953a-45a9-bc5d-22157cac86fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
